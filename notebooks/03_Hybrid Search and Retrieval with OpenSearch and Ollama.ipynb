{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6ada44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import sys\n",
    "from typing import Dict, Any, List, Optional, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import ollama\n",
    "from opensearchpy import OpenSearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set up Python path to access project modules\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import from project modules\n",
    "from src.constants import (\n",
    "    ASSYMETRIC_EMBEDDING,\n",
    "    OLLAMA_MODEL_NAME,\n",
    "    OPENSEARCH_HOST,\n",
    "    OPENSEARCH_PORT,\n",
    "    OPENSEARCH_INDEX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b3deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding settings\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Model for generating embeddings\n",
    "EMBEDDING_DIMENSION = 384  # Embedding dimension for the model\n",
    "ASSYMETRIC_EMBEDDING = False  # Whether to use asymmetric embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597022b",
   "metadata": {},
   "source": [
    "# 1. Connect to OpenSearch and Set Up Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73696f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to OpenSearch 2.19.2\n"
     ]
    }
   ],
   "source": [
    "client = OpenSearch(\n",
    "    hosts = [{\"host\": OPENSEARCH_HOST, \"port\": OPENSEARCH_PORT}],\n",
    "    http_compress = True,\n",
    "    timeout = 30,\n",
    "    max_retries = 3,\n",
    "    retry_on_timeout = True\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    info = client.info()\n",
    "    print(f\"Successfully connected to OpenSearch {info['version']['number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to OpenSearch: {e}\")\n",
    "    print(\"Make sure OpenSearch is running on localhost:9200\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad8ba2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Search pipeline 'nlp-search-pipeline' exists.\n"
     ]
    }
   ],
   "source": [
    "# Verify pipeline exists\n",
    "from opensearchpy.exceptions import NotFoundError\n",
    "pipeline_name = \"nlp-search-pipeline\"\n",
    "\n",
    "try:\n",
    "    result = client.transport.perform_request(\n",
    "        \"GET\",\n",
    "        f\"/_search/pipeline/{pipeline_name}\"\n",
    "    )\n",
    "    print(f\"\\n✅ Search pipeline '{pipeline_name}' exists.\")\n",
    "except NotFoundError:\n",
    "    print(f\"\\n⚠️ Search pipeline '{pipeline_name}' does NOT exist.\")\n",
    "    print(\"This is required for hybrid search. Please run the prerequisites notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n🚨 Error: {e}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d06f63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query_text: str, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Performs hybrid search combining text-based and vector-based queries.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The text query for BM25 search\n",
    "        query_embedding (List[float]): The vector embedding for KNN search\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: The search results\n",
    "    \"\"\"\n",
    "    query_body = {\n",
    "        \"_source\": {\"exclude\": [\"embedding\"]},  # Exclude embeddings from results\n",
    "        \"query\": {\n",
    "            \"hybrid\": {\n",
    "                \"queries\": [\n",
    "                    {\"match\": {\"text\": {\"query\": query_text}}},  # Text-based search\n",
    "                    {\n",
    "                        \"knn\": {\n",
    "                            \"embedding\": {\n",
    "                                \"vector\": query_embedding,\n",
    "                                \"k\": top_k,\n",
    "                            }\n",
    "                        }\n",
    "                    },  # Vector-based search\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k,\n",
    "    }\n",
    "    \n",
    "    print(\"\\nExecuting hybrid search query...\")\n",
    "    try:\n",
    "        # Try with search pipeline parameter (for newer OpenSearch versions)\n",
    "        response = client.search(\n",
    "            index=OPENSEARCH_INDEX,\n",
    "            body=query_body,\n",
    "            params={\"search_pipeline\": \"nlp-search-pipeline\"} # Uses the pipeline for score normalization\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Fall back to without pipeline parameter for older versions\n",
    "        print(\"Warning: OpenSearch client doesn't support search_pipeline parameter, using raw query\")\n",
    "        response = client.search(\n",
    "            index=OPENSEARCH_INDEX,\n",
    "            body=query_body\n",
    "        )\n",
    "    \n",
    "    return response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc3811",
   "metadata": {},
   "source": [
    "# 2. Process Query and Perform Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5e4c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model():\n",
    "    \"\"\"\n",
    "    Loads and returns the sentence transformer embedding model.\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer: The loaded embedding model.\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_embeddings(texts: List[str]):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of text chunks.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text chunks to embed.\n",
    "        \n",
    "    Returns:\n",
    "        List[numpy.ndarray]: List of embedding vectors.\n",
    "    \"\"\"\n",
    "    model = get_embedding_model()\n",
    "    \n",
    "    # If using asymmetric embeddings, prefix each text with \"passage: \"\n",
    "    if ASSYMETRIC_EMBEDDING:\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3179cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What is the average rate of ice loss'\n",
      "\n",
      "Generating embedding for query...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Generated embedding with dimension: 384\n",
      "\n",
      "Retrieving top 3 documents...\n",
      "\n",
      "Executing hybrid search query...\n",
      "\n",
      "Search results for query: 'What is the average rate of ice loss'\n",
      "\n",
      "Result 1 (Score: 0.700):\n",
      "Text: Gt yr−1 of ice loss is equivalent to about 0.28 mm yr−1 of global mean sea level rise.SPMSummary for Policymakers101900 1920 1940 1960 1980 2000−20−1001020 Year (1022 J)Change in global average upper ...\n",
      "Document: climate\n",
      "\n",
      "Result 2 (Score: 0.525):\n",
      "Text: km2 per decade), and very likely in the range 9.4 to 13.6% per decade (range of 0.73 to 1.07 million km2 per decade) for the summer sea ice minimum (perennial sea ice). The average decrease in decadal...\n",
      "Document: climate\n",
      "\n",
      "Result 3 (Score: 0.300):\n",
      "Text: evaporation and precipitation over the oceans have changed ( medium confidence ). {2.5, 3.3, 3.5} • There is no observational evidence of a trend in the Atlantic Meridional Overturning Circulation (AM...\n",
      "Document: climate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample query\n",
    "query = \"What is the average rate of ice loss\"\n",
    "print(f\"Query: '{query}'\")\n",
    "\n",
    "# Generate query embedding\n",
    "print(\"\\nGenerating embedding for query...\")\n",
    "embeddings = generate_embeddings([query])\n",
    "query_embedding = embeddings[0].tolist()\n",
    "print(f\"Generated embedding with dimension: {len(query_embedding)}\")\n",
    "\n",
    "# Set number of results to retrieve\n",
    "top_k = 3\n",
    "print(f\"\\nRetrieving top {top_k} documents...\")\n",
    "\n",
    "# Perform hybrid search\n",
    "results = hybrid_search(query, query_embedding, top_k=top_k)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nSearch results for query: '{query}'\\n\")\n",
    "for i, hit in enumerate(results, 1):\n",
    "    print(f\"Result {i} (Score: {hit['_score']:.3f}):\")\n",
    "    print(f\"Text: {hit['_source']['text'][:200]}...\")  # Showing truncated text\n",
    "    print(f\"Document: {hit['_source']['document_name']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b772ce",
   "metadata": {},
   "source": [
    "# 3. Generate Response with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad5a12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate responses with Ollama\n",
    "def generate_response_with_ollama(query: str, results: List[Dict], model_name: str = OLLAMA_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Generates a response using Ollama based on search results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        results (List[Dict]): The search results from OpenSearch\n",
    "        model_name (str): The Ollama model to use\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing (prompt, model_name)\n",
    "    \"\"\"\n",
    "    # Format context from search results\n",
    "    context = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        context += f\"Document {i + 1}:\\n{result['_source']['text']}\\n\\n\"\n",
    "\n",
    "    # Create prompt template\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "If you cannot find the answer in the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "    return prompt, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35cb9a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring Ollama model llama3.2:1b is available...\n",
      "Model llama3.2:1b is ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure model is pulled\n",
    "print(f\"Ensuring Ollama model {OLLAMA_MODEL_NAME} is available...\")\n",
    "try:\n",
    "    ollama.pull(OLLAMA_MODEL_NAME)\n",
    "    print(f\"Model {OLLAMA_MODEL_NAME} is ready.\")\n",
    "except ollama.ResponseError as e:\n",
    "    print(f\"Error pulling model: {e.error}\")\n",
    "    print(\"You might need to install the model manually with: ollama pull \" + OLLAMA_MODEL_NAME)\n",
    "\n",
    "# Get prompt and model\n",
    "prompt, model_name = generate_response_with_ollama(query, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3fda563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt created with 9189 characters\n",
      "First 200 characters of prompt:\n",
      "You are a helpful AI assistant. Use the following context to answer the question.\n",
      "If you cannot find the answer in the context, say so.\n",
      "\n",
      "Context:\n",
      "Document 1:\n",
      "Gt yr−1 of ice loss is equivalent to about...\n",
      "\n",
      "Generating response with Ollama...\n",
      "\n",
      "Response:\n",
      "The question asks for the average rate of ice loss from glaciers around the world, excluding glaciers on the periphery of the ice sheets. According to Document 3, the average rate of ice loss was very likely:\n",
      "\n",
      "* From 1971 to 2009: 226 Gt yr−1\n",
      "* From 1993 to 2009: 275 Gt yr−1\n",
      "\n",
      "However, it's worth noting that these values are based on limited data and are considered high confidence levels. The actual rates of ice loss may be different or more variable.\n",
      "\n",
      "Response generation complete!\n",
      "Generated 454 characters\n"
     ]
    }
   ],
   "source": [
    "# Print prompt length\n",
    "print(f\"\\nPrompt created with {len(prompt)} characters\")\n",
    "print(\"First 200 characters of prompt:\")\n",
    "print(prompt[:200] + \"...\")\n",
    "\n",
    "# Generate streaming response\n",
    "print(\"\\nGenerating response with Ollama...\")\n",
    "response = \"\"\n",
    "print(\"\\nResponse:\")\n",
    "for chunk in ollama.generate(model=model_name, prompt=prompt, stream=True):\n",
    "    piece = chunk['response']\n",
    "    print(piece, end='', flush=True)\n",
    "    response += piece\n",
    "\n",
    "print(\"\\n\\nResponse generation complete!\")\n",
    "print(f\"Generated {len(response)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348f3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
